{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95966f42",
   "metadata": {},
   "source": [
    "# Taxi Fare Prediction Model_Feature_Engineering&_EDA\n",
    "\n",
    "## Introduction\n",
    "This project aims to develop a predictive model for taxi fares in NYC. Initially, we will create a model for NYC and adjust parameters to align with domain knowledge from Tbilisi. We hypothesize that factors such as time of day, seasonality, and holidays impact taxi demand and fare prices.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8404b190",
   "metadata": {},
   "source": [
    "### Notebook Aim and Feature Development\n",
    "\n",
    "#### **Objective**\n",
    "The primary objective of our project is to develop a predictive model that accurately forecasts taxi fares. Initially focusing on New York City (NYC), we aim to expand and adapt the model to incorporate Tbilisi, employing localized domain knowledge to tailor our approach.\n",
    "\n",
    "#### **Influence of Demand on Pricing**\n",
    "The fare prices in the taxi industry are predominantly influenced by demand dynamics, which can fluctuate based on various factors including:\n",
    "\n",
    "- **Seasons**: Investigating how seasonal changes—spring, summer, autumn, and winter—affect taxi demand and subsequently, fare prices.\n",
    "- **Day of the Week**: Determining if there are variations in taxi usage and prices between weekdays and weekends.\n",
    "- **Time of Day**: Analyzing how time segments (morning, afternoon, evening, and night) impact traffic conditions and fare rates, particularly during peak rush hours.\n",
    "- **Holidays**: Assessing the effect of major holidays (e.g., Christmas, Thanksgiving) on taxi demand, given the potential increase in tourism and local activity.\n",
    "\n",
    "#### **Additional Influential Features**\n",
    "Beyond temporal and periodic factors, several other elements could influence fare pricing:\n",
    "\n",
    "- **Passenger Count**: Exploring whether vehicles accommodating more passengers have different fare structures, similar to practices in ride-sharing applications.\n",
    "- **Trip Distance and Duration**: Both metrics are crucial for pricing. While trip distance is a direct influencer, the duration might also affect costs, especially in varying traffic conditions.\n",
    "- **Velocity**: By calculating the average speed of a trip (velocity = distance/duration), we can examine if faster trips result in different pricing.\n",
    "- **Taxi Zones**: With the NYC taxi zone dataset, we can analyze whether specific pickup and dropoff locations impact fare prices due to their geographical significance.\n",
    "\n",
    "\n",
    "### Summary\n",
    "This comprehensive approach not only allows us to understand the multifaceted dynamics of taxi fare pricing in NYC but also sets a foundation for adapting the model to Tbilisi, ensuring that both city-specific and universal factors are considered for effective fare prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ba67ca",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "In this section we will install necessary packages, imports necessary libraries and load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218e99ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyarrow\n",
    "!pip install fastparquet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ea1ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar as calande"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff700eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'path_to_file.parquet' with the path to your Parquet file\n",
    "df_original = pd.read_parquet('/Users/md/Desktop/python_project/parquet_files/cleaned/cleaned_taxi_data.parquet', engine='pyarrow')  # or engine='fastparquet' if you prefer\n",
    "df = df_original"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4d7201",
   "metadata": {},
   "source": [
    "Let's check for data accuracy and that the cleaned data is clean and has all the columns after data preparation in previous notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3c76b0",
   "metadata": {},
   "source": [
    "## Data Initial View and Celaining \n",
    "\n",
    "As we already process cleaned data from previous notebook we do not need to clean the data for nulls, duplicates or outliers however to check that data is consistent and clean we will have an initial look at the laoded dataset below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61d6ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9297faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e74ab5",
   "metadata": {},
   "source": [
    "### Additional Dataset For Taxi Zones Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8e541a",
   "metadata": {},
   "outputs": [],
   "source": [
    "zones = pd.read_csv(\"/Users/md/Desktop/python_project/parquet_files/cleaned/taxi_zones.csv\", sep=';')\n",
    "zones.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5d011d",
   "metadata": {},
   "outputs": [],
   "source": [
    "zones.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cf1384",
   "metadata": {},
   "source": [
    "we already see that we have 263 zones, in our dataset we have 265 zones for taxis, which means we already know that when joining we will have to adjust for missing values and try to find this zones or remove them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd7b497",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "Below based on our domain knowledge and literature reviews we will create new features or adjust the existing ones to gain more insights on the data and cerate best possible predictive model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933186f4",
   "metadata": {},
   "source": [
    "## Seasonal and Time Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884aae9f",
   "metadata": {},
   "source": [
    "\n",
    "- **Seasons**: Investigating how seasonal changes—spring, summer, autumn, and winter—affect taxi demand and subsequently, fare prices.\n",
    "- **Day of the Week**: Determining if there are variations in taxi usage and prices between weekdays and weekends.\n",
    "- **Time of Day**: Analyzing how time segments (morning, afternoon, evening, and night) impact traffic conditions and fare rates, particularly during peak rush hours.\n",
    "- **Duration**: How long did the trip last."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43acc0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the pickup and dropoff datetime to pandas datetime format if not already\n",
    "df['tpep_pickup_datetime'] = pd.to_datetime(df['tpep_pickup_datetime'])\n",
    "df['tpep_dropoff_datetime'] = pd.to_datetime(df['tpep_dropoff_datetime'])\n",
    "\n",
    "# Time of day segmentation\n",
    "df['pickup_time_of_day'] = df['tpep_pickup_datetime'].dt.hour.apply(lambda x: 'morning' if 5 <= x <= 11\n",
    "                                                                           else 'afternoon' if 12 <= x <= 17\n",
    "                                                                           else 'evening' if 18 <= x <= 23\n",
    "                                                                           else 'night')\n",
    "\n",
    "# Seasons segmentation\n",
    "df['pickup_season'] = df['tpep_pickup_datetime'].dt.month.apply(lambda x: 'spring' if 3 <= x <= 5\n",
    "                                                                       else 'summer' if 6 <= x <= 8\n",
    "                                                                       else 'autumn' if 9 <= x <= 11\n",
    "                                                                       else 'winter')\n",
    "\n",
    "# Passenger count categories\n",
    "df['passenger_count_category'] = pd.cut(df['passenger_count'], bins=[0, 1, 4, 6], include_lowest=True, \n",
    "                                        labels=['low', 'medium', 'high'])\n",
    "\n",
    "# Weekday/Weekend segmentation\n",
    "df['pickup_day_type'] = df['tpep_pickup_datetime'].dt.day_name().apply(lambda x: 'weekend' if x in ['Saturday', 'Sunday'] else 'weekday')\n",
    "\n",
    "\n",
    "#taxi_data_prepared['transaction_date'] = pd.to_datetime(taxi_data_prepared['tpep_pickup_datetime'].dt.date)\n",
    "# -> we make it datetime again because it's very little use when it's just a string (can't compare, sort, etc.)\n",
    "df['transaction_year'] = df['tpep_pickup_datetime'].dt.year\n",
    "df['transaction_month'] = df['tpep_pickup_datetime'].dt.month\n",
    "df['transaction_day'] =  df['tpep_pickup_datetime'].dt.day\n",
    "df['transaction_hour'] = df['tpep_pickup_datetime'].dt.hour\n",
    "\n",
    "#trip duration is another interesting feature to analyze \n",
    "\n",
    "\n",
    "# Calculate the trip duration and convert it to minutes\n",
    "df['trip_duration'] = (df['tpep_dropoff_datetime'] - df['tpep_pickup_datetime']).dt.total_seconds() / 60\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea1ec72",
   "metadata": {},
   "source": [
    "Lets take a look at adjusted dataset and what are the new created features we will sample the dataset to also test that the fatures were created correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa47ca30",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484c221f",
   "metadata": {},
   "source": [
    "Below we will check if the trip duration calculations are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea12c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows to confirm the new 'trip_duration' column\n",
    "print(df[['tpep_pickup_datetime', 'tpep_dropoff_datetime', 'trip_duration']].sample(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d092def",
   "metadata": {},
   "source": [
    "To check if newly added features have correct values we will use descriptive statistics and adjust accordingly if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fdf135",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692db891",
   "metadata": {},
   "source": [
    "We see that for month, year, day, season features, the values make sense although for trip duration we can see that we have negative trip durations. \n",
    "\n",
    "\n",
    "Negative trip durations may have occured due to data entry issues , times might have been mixed up. we can investigate further and see what is the number of negative values and either drop the corrupted data or adjust it accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a6c94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display cases with negative trip_duration\n",
    "negative_durations = df[df['trip_duration'] < 0]\n",
    "negative_durations[['tpep_pickup_datetime', 'tpep_dropoff_datetime', 'trip_duration']]\n",
    "\n",
    "negative_durations.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1c4c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for possible datetime swaps or errors\n",
    "swapped_cases = df[df['tpep_pickup_datetime'] > df['tpep_dropoff_datetime']]\n",
    "print(swapped_cases[['tpep_pickup_datetime', 'tpep_dropoff_datetime', 'trip_duration']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220ae5b2",
   "metadata": {},
   "source": [
    "as we can see there are only 727 negative values which compared to full dataset is really low number thus instead of going over 30million records to witch the rows we will drop rows with. trip durations less than or equal to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0daf1955",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['trip_duration']>0]\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f43ae6",
   "metadata": {},
   "source": [
    "## Taxi Zones_ Feature \n",
    "taxi zone ID s though informative they do not provide any insights as to where passanger was picked up and neighbourhoods are thought to effect pricing at least when hailing a cab thus we will merge Taxi zone dataset with the NYC trip data on zone IDs and idnetify pickup and drop off buroughs for each trip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617f7eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the zone data into the main taxi trip dataset for pickup locations\n",
    "df = pd.merge(df, zones[['LocationID', 'zone', 'borough']], left_on='PULocationID', right_on='LocationID', how='left')\n",
    "df.rename(columns={'zone': 'PUzone', 'borough': 'PUborough'}, inplace=True)\n",
    "\n",
    "# Merge the zone data for dropoff locations\n",
    "df = pd.merge(df, zones[['LocationID', 'zone', 'borough']], left_on='DOLocationID', right_on='LocationID', how='left', suffixes=('', '_drop'))\n",
    "df.rename(columns={'zone': 'DOzone', 'borough': 'DOborough'}, inplace=True)\n",
    "\n",
    "# Drop the extra LocationID columns if they are not needed\n",
    "df.drop(['LocationID', 'LocationID_drop'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb2e6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['PUborough'].value_counts())\n",
    "print(df['DOborough'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfd5e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[['PUzone', 'PUborough', 'DOzone', 'DOborough']].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4dea4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted(zones['LocationID'].unique()))\n",
    "print(sorted(df['PULocationID'].unique()))\n",
    "print(sorted(df['DOLocationID'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a947bf",
   "metadata": {},
   "source": [
    "We can see that in our dataset we have 2 zones namely 264 and 265 which do not have specific buroughs and are not in our taxi zones dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdb14c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_pu = df[~df['PULocationID'].isin(zones['LocationID'])]\n",
    "missing_do = df[~df['DOLocationID'].isin(zones['LocationID'])]\n",
    "print(f\"Missing PULocationIDs: {missing_pu['PULocationID'].unique()}\")\n",
    "print(f\"Missing DOLocationIDs: {missing_do['DOLocationID'].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b8d0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data for PULocationID or DOLocationID being 264 or 265\n",
    "trips = df[(df['PULocationID'].isin([264, 265])) | (df['DOLocationID'].isin([264, 265]))]\n",
    "\n",
    "# Print the filtered data summary\n",
    "trips.describe(include='all')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849eaf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample records\n",
    "trips.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d622b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually assign zones for IDs 264 and 265\n",
    "df.loc[df['PULocationID'] == 264, ['PUzone', 'PUborough']] = ['Outside NYC', 'Unknown']\n",
    "df.loc[df['DOLocationID'] == 264, ['DOzone', 'DOborough']] = ['Outside NYC', 'Unknown']\n",
    "df.loc[df['PULocationID'] == 265, ['PUzone', 'PUborough']] = ['Airport Area', 'Unknown']\n",
    "df.loc[df['DOLocationID'] == 265, ['DOzone', 'DOborough']] = ['Airport Area', 'Unknown']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6fc9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for null values in the updated columns\n",
    "print(df[['PUzone', 'PUborough', 'DOzone', 'DOborough']].isnull().sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0039e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print rows where PUzone or PUborough is null\n",
    "print(\"Rows with missing PUzone or PUborough:\")\n",
    "print(df[df['PUzone'].isnull() | df['PUborough'].isnull()][['PULocationID', 'PUzone', 'PUborough']].head())\n",
    "\n",
    "# Print rows where DOzone or DOborough is null\n",
    "print(\"Rows with missing DOzone or DOborough:\")\n",
    "print(df[df['DOzone'].isnull() | df['DOborough'].isnull()][['DOLocationID', 'DOzone', 'DOborough']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d92001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List unique LocationIDs associated with null zones or boroughs\n",
    "missing_pu_ids = df[df['PUzone'].isnull()]['PULocationID'].unique()\n",
    "missing_do_ids = df[df['DOzone'].isnull()]['DOLocationID'].unique()\n",
    "print(f\"Missing PULocationIDs: {missing_pu_ids}\")\n",
    "print(f\"Missing DOLocationIDs: {missing_do_ids}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595e0ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually assign zones and boroughs for LocationID 57 and 105\n",
    "df.loc[df['PULocationID'] == 57, ['PUzone', 'PUborough']] = ['Corona', 'Queens']\n",
    "df.loc[df['DOLocationID'] == 57, ['DOzone', 'DOborough']] = ['Corona', 'Queens']\n",
    "\n",
    "df.loc[df['PULocationID'] == 105, ['PUzone', 'PUborough']] = [\"Governor's Island/Ellis Island/Liberty Island\", 'Manhattan']\n",
    "df.loc[df['DOLocationID'] == 105, ['DOzone', 'DOborough']] = [\"Governor's Island/Ellis Island/Liberty Island\", 'Manhattan']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e76330",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Verify updates for LocationID 57\n",
    "print(\"Updated zones and boroughs for LocationID 57:\")\n",
    "print(df[df['PULocationID'] == 57][['PULocationID', 'PUzone', 'PUborough']].head(2))\n",
    "print(df[df['DOLocationID'] == 57][['DOLocationID', 'DOzone', 'DOborough']].head(2))\n",
    "\n",
    "# Verify updates for LocationID 105\n",
    "print(\"Updated zones and boroughs for LocationID 105:\")\n",
    "print(df[df['PULocationID'] == 105][['PULocationID', 'PUzone', 'PUborough']].head(2))\n",
    "print(df[df['DOLocationID'] == 105][['DOLocationID', 'DOzone', 'DOborough']].head(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3afedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check again for null values in the zone and borough columns\n",
    "print(\"Null values in PUzone and PUborough after update:\")\n",
    "print(df[['PUzone', 'PUborough']].isnull().sum())\n",
    "\n",
    "print(\"Null values in DOzone and DOborough after update:\")\n",
    "print(df[['DOzone', 'DOborough']].isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0ef853",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465cedd3",
   "metadata": {},
   "source": [
    "## Holiday\n",
    "- **Holidays**: Assessing the effect of major holidays (e.g., Christmas, Thanksgiving) on taxi demand, given the potential increase in tourism and local activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7607edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a calendar object\n",
    "calendar = USFederalHolidayCalendar()\n",
    "\n",
    "# Define the range for your data\n",
    "start_date = '2023-01-01'\n",
    "end_date = '2023-12-31'\n",
    "\n",
    "# Generate holidays\n",
    "holidays = calendar.holidays(start=start_date, end=end_date)\n",
    "\n",
    "# Add a column to your dataframe indicating whether the trip started on a holiday\n",
    "df['is_holiday'] = df['tpep_pickup_datetime'].dt.normalize().isin(holidays).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8eb9ab",
   "metadata": {},
   "source": [
    "## Velocity\n",
    "\n",
    "**Velocity**: By calculating the average speed of a trip (velocity = distance/duration), we can examine if faster trips result in different pricing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6077d7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, ensure your trip_duration is in hours for speed calculation\n",
    "df['trip_duration_hours'] = df['trip_duration'] / 60.0\n",
    "\n",
    "# Calculate speed\n",
    "df['speed_mph'] = df['trip_distance'] / df['trip_duration_hours']\n",
    "\n",
    "# Handle any potential infinite or NaN values that may occur if duration is zero\n",
    "df['speed_mph'].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df['speed_mph'].fillna(0, inplace=True)  # Optionally set to zero or another placeholder value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c706ca2",
   "metadata": {},
   "source": [
    "## Testing NEW Feature Validity\n",
    "\n",
    "We need to check if created features are within their bounds and our code worked properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39707e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_trip_duration_positive():\n",
    "    assert df['trip_duration'].min() > 0, \"Error: Non-positive trip durations present in the dataset.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42aa3742",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_time_of_day_categories():\n",
    "    hours = df['tpep_pickup_datetime'].dt.hour\n",
    "    conditions = [\n",
    "        ((hours >= 5) & (hours <= 11)),\n",
    "        ((hours >= 12) & (hours <= 17)),\n",
    "        ((hours >= 18) & (hours <= 23)),\n",
    "        ((hours < 5) | (hours == 24))\n",
    "    ]\n",
    "    categories = ['morning', 'afternoon', 'evening', 'night']\n",
    "    for condition, category in zip(conditions, categories):\n",
    "        assert all(df.loc[condition, 'pickup_time_of_day'] == category), f\"Error in categorizing {category}.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272300d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_passenger_count_categories():\n",
    "    conditions = [\n",
    "        (df['passenger_count'] == 1),\n",
    "        (df['passenger_count'].between(2, 4)),\n",
    "        (df['passenger_count'].between(5, 6))\n",
    "    ]\n",
    "    categories = ['low', 'medium', 'high']\n",
    "    for condition, category in zip(conditions, categories):\n",
    "        assert all(df.loc[condition, 'passenger_count_category'] == category), f\"Error in categorizing passenger count {category}.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2ac483",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_seasonal_categories():\n",
    "    months = df['tpep_pickup_datetime'].dt.month\n",
    "    conditions = [\n",
    "        (months.isin([3, 4, 5])),\n",
    "        (months.isin([6, 7, 8])),\n",
    "        (months.isin([9, 10, 11])),\n",
    "        (months.isin([12, 1, 2]))\n",
    "    ]\n",
    "    seasons = ['spring', 'summer', 'autumn', 'winter']\n",
    "    for condition, season in zip(conditions, seasons):\n",
    "        assert all(df.loc[condition, 'pickup_season'] == season), f\"Error in season categorization for {season}.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a057cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
    "\n",
    "def test_holiday_flag():\n",
    "    calendar = USFederalHolidayCalendar()\n",
    "    holidays = calendar.holidays(start=df['tpep_pickup_datetime'].min(), end=df['tpep_pickup_datetime'].max())\n",
    "    df['calculated_holiday'] = df['tpep_pickup_datetime'].dt.normalize().isin(holidays).astype(int)\n",
    "    assert all(df['calculated_holiday'] == df['is_holiday']), \"Holiday flag mismatches detected.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053f28d5",
   "metadata": {},
   "source": [
    "# Selecting Needed Features \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0894ab",
   "metadata": {},
   "source": [
    "### Conducting Exploratory Data Analysis (EDA) for NYC Taxi Trips\n",
    "\n",
    "Now that you've cleaned the dataset and engineered relevant features, the next logical step is to conduct an Exploratory Data Analysis (EDA). This will help you to:\n",
    "\n",
    "1. **Understand the Distribution and Relationships**:\n",
    "   - **Visualize distributions** of individual variables using histograms, box plots, and density plots to understand their range, central values, and variance.\n",
    "   - **Explore relationships** between variables using scatter plots, pair plots, and correlation matrices. This helps in understanding how different features such as trip distance, duration, time of day, and passenger count relate to fare amounts.\n",
    "\n",
    "2. **Identify Patterns and Anomalies**:\n",
    "   - Look for patterns that may indicate how time of day or day of the week affects taxi usage and fare costs.\n",
    "   - Identify potential outliers or anomalies that weren't previously detected during the cleaning process.\n",
    "\n",
    "3. **Statistical Analysis**:\n",
    "   - Apply statistical tests to confirm hypotheses about the data, for example, testing if average fares are significantly different during holidays or specific seasons.\n",
    "   - Use aggregation to see mean, median, and standard deviation of fares across different times or zones.\n",
    "\n",
    "4. **Geographical Insights**:\n",
    "   - Utilize geographical plotting to visualize trips by pickup and drop-off locations. This can highlight busy areas or zones with higher fares.\n",
    "   - Analyze how the geographic factors unique to NYC might translate to Tbilisi, considering urban layout differences.\n",
    "\n",
    "5. **Temporal Dynamics**:\n",
    "   - Investigate how different times of the year, week, or day impact fares and trip frequencies.\n",
    "   - Determine if there's a \"rush hour\" effect in NYC and predict potential similar effects in Tbilisi.\n",
    "\n",
    "### Drawing Conclusions for Model Development\n",
    "\n",
    "After EDA, you should be able to draw preliminary conclusions about:\n",
    "\n",
    "- **Key Drivers of Fare Prices**: Understanding which features are most predictive of fare changes can guide the feature selection for your predictive model.\n",
    "- **Transferability of Insights**: Evaluate which aspects of the NYC taxi fare dynamics are likely to apply to Tbilisi. Consider socio-economic, geographical, and cultural differences that could influence model adjustments.\n",
    "- **Data Quality and Further Needs**: Determine if additional data or further cleaning is required based on findings from EDA. For example, weather conditions or special events data could enhance the model.\n",
    "\n",
    "### Moving Forward\n",
    "\n",
    "With insights gained from EDA, you can proceed to:\n",
    "- **Feature Selection**: Decide which features to include in your predictive model based on their relevance and impact on taxi fares.\n",
    "- **Model Building**: Begin with simple models to establish a baseline before experimenting with more complex models like Random Forests or Gradient Boosting Machines.\n",
    "- **Cross-Validation and Hyperparameter Tuning**: Implement these strategies to optimize model performance and ensure it generalizes well on unseen data.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "The comprehensive EDA will not only refine your understanding of NYC taxi trips but also equip you with the insights needed to tackle the fare prediction challenge in Tbilisi, adapting the approach based on localized conditions and data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c32f9f",
   "metadata": {},
   "source": [
    "Great! Let's approach the Exploratory Data Analysis (EDA) systematically to refine your dataset and prepare for modeling. Here’s a structured plan to conduct EDA and feature refinement:\n",
    "\n",
    "### 1. **Visual Analysis of Distribution and Relationships**\n",
    "   - **Univariate Analysis**: Use histograms and box plots to analyze the distribution of continuous features like `fare_amount`, `trip_distance`, and `trip_duration`. Bar charts might be useful for categorical features like `pickup_day_type` and `passenger_count_category`.\n",
    "   - **Bivariate Analysis**: Create scatter plots to examine relationships between fare and other numerical features. Use heatmaps to visualize correlations.\n",
    "\n",
    "### 2. **Identify and Handle Outliers**\n",
    "   - Review the plots to identify outliers. Decide if they are data errors or just extreme values. Remove or cap them as necessary, depending on their authenticity and impact on the model.\n",
    "   - For example, trips with unusually long durations or distances that don't correspond to higher fares might be errors.\n",
    "\n",
    "### 3. **Geographical Insights**\n",
    "   - If geographical data is available, plot the pickup and drop-off locations using a scatter geo plot to identify high-traffic areas and fare patterns.\n",
    "   - Consider whether geographical features relevant to NYC will be applicable in Tbilisi or if adjustments are needed.\n",
    "\n",
    "### 4. **Temporal Analysis**\n",
    "   - Examine how fares vary by time of day, day of the week, and season. This can uncover demand patterns which are crucial for dynamic pricing models.\n",
    "   - Plot time-series of fares over the months to detect any trends or seasonal effects.\n",
    "\n",
    "### 5. **Categorical and Dummy Variable Creation**\n",
    "   - Convert categorical variables into dummy variables if needed, for modeling purposes. This includes variables like `pickup_day_type`, `pickup_time_of_day`, and `season`.\n",
    "   - Make sure to drop one dummy variable to avoid multicollinearity.\n",
    "\n",
    "### 6. **Redundancy and Irrelevance Check**\n",
    "   - Remove or combine features that are redundant or have little impact on the fare. For instance, if `pickup_day_type` (weekend vs. weekday) captures the essence of `transaction_day`, consider dropping one.\n",
    "   - Evaluate the necessity of features like `store_and_fwd_flag` if they do not significantly impact fare.\n",
    "\n",
    "### 7. **Feature Engineering Reassessment**\n",
    "   - Revisit your engineered features based on insights gained during EDA. Adjust or create new features as needed.\n",
    "   - Consider engineering velocity or efficiency features like `speed` (distance over duration).\n",
    "\n",
    "### 8. **Final Dataset Preparation**\n",
    "   - Ensure all data types are correct and that missing values are handled.\n",
    "   - Normalize or scale features if necessary, depending on the chosen modeling techniques.\n",
    "   - Split the dataset into training and testing sets to prepare for modeling.\n",
    "\n",
    "### 9. **Data Export**\n",
    "   - Save the cleaned and transformed dataset to a new file, ready for use in model training and validation in your next notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6beec3",
   "metadata": {},
   "source": [
    "## Univariate Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee03f5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Setting aesthetic style for seaborn plots\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Histogram for trip distances\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df['trip_distance'], bins=30, kde=True)\n",
    "plt.title('Distribution of Trip Distances')\n",
    "plt.xlabel('Trip Distance (miles)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Boxplot for fare amounts\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x=df['fare_amount'])\n",
    "plt.title('Box Plot of Fare Amounts')\n",
    "plt.xlabel('Fare Amount ($)')\n",
    "plt.show()\n",
    "\n",
    "# Count plot for day of the week\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x='pickup_day_type', data=df)\n",
    "plt.title('Trip Counts by Day Type')\n",
    "plt.xlabel('Day Type')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# Count plot for time of day\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x='pickup_time_of_day', data=df)\n",
    "plt.title('Trip Counts by Time of Day')\n",
    "plt.xlabel('Time of Day')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b0b0d5",
   "metadata": {},
   "source": [
    "## Demand Time Of Day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29ebc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sns.countplot(x='pickup_time_of_day', data=df, order=['morning', 'afternoon', 'evening', 'night'])\n",
    "plt.title('Demand by Time of Day')\n",
    "plt.xlabel('Time of Day')\n",
    "plt.ylabel('Number of Trips')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f148bd3",
   "metadata": {},
   "source": [
    "## Demand Day Of The Week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9cae269",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sns.countplot(x='pickup_day_type', data=df, order=['weekday', 'weekend'])\n",
    "plt.title('Demand by Day of the Week')\n",
    "plt.xlabel('Day of the Week')\n",
    "plt.ylabel('Number of Trips')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f15bed4",
   "metadata": {},
   "source": [
    "## Demand By seaoson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e64f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sns.countplot(x='pickup_season', data=df, order=['spring', 'summer', 'autumn', 'winter'])\n",
    "plt.title('Demand by Season')\n",
    "plt.xlabel('Season')\n",
    "plt.ylabel('Number of Trips')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ffa1f6",
   "metadata": {},
   "source": [
    "## Hourly Demand "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0958df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new column for the day of the week\n",
    "df['pickup_day_of_week'] = df['tpep_pickup_datetime'].dt.day_name()\n",
    "\n",
    "# Creating a pivot table for the heatmap\n",
    "pivot = df.pivot_table(index='pickup_day_of_week', columns='pickup_time_of_day', values='fare_amount', aggfunc='count')\n",
    "\n",
    "# Ordering the days correctly for the heatmap\n",
    "ordered_days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "pivot = pivot.reindex(ordered_days)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(pivot, cmap=\"YlGnBu\", linewidths=.5, annot=True, fmt=\".0f\")\n",
    "plt.title('Hourly Demand Throughout the Week')\n",
    "plt.xlabel('Time of Day')\n",
    "plt.ylabel('Day of the Week')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948040ce",
   "metadata": {},
   "source": [
    "## Bivariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a07a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot for trip distance vs fare amount\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='trip_distance', y='fare_amount', data=df)\n",
    "plt.title('Trip Distance vs. Fare Amount')\n",
    "plt.xlabel('Trip Distance (miles)')\n",
    "plt.ylabel('Fare Amount ($)')\n",
    "plt.show()\n",
    "\n",
    "# Boxplot of fare amount by day of the week\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='pickup_day_type', y='fare_amount', data=df)\n",
    "plt.title('Fare Amount by Day Type')\n",
    "plt.xlabel('Day Type')\n",
    "plt.ylabel('Fare Amount ($)')\n",
    "plt.show()\n",
    "\n",
    "# Boxplot of fare amount by time of day\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='pickup_time_of_day', y='fare_amount', data=df)\n",
    "plt.title('Fare Amount by Time of Day')\n",
    "plt.xlabel('Time of Day')\n",
    "plt.ylabel('Fare Amount ($)')\n",
    "plt.show()\n",
    "\n",
    "# Boxplot of fare amount by passenger count category\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='passenger_count_category', y='fare_amount', data=df)\n",
    "plt.title('Fare Amount by Passenger Count Category')\n",
    "plt.xlabel('Passenger Count Category')\n",
    "plt.ylabel('Fare Amount ($)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c078a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if 'pickup_season' is correctly categorized\n",
    "season_counts = df['pickup_season'].value_counts()\n",
    "print(season_counts)\n",
    "\n",
    "# Check if 'pickup_time_of_day' is correctly categorized\n",
    "time_of_day_counts = df['pickup_time_of_day'].value_counts()\n",
    "print(time_of_day_counts)\n",
    "\n",
    "# Check if 'pickup_day_type' is correctly categorized for weekdays and weekends\n",
    "day_type_counts = df['pickup_day_type'].value_counts()\n",
    "print(day_type_counts)\n",
    "\n",
    "# To check all together\n",
    "summary = df.groupby(['pickup_day_type', 'pickup_time_of_day', 'pickup_season'])['fare_amount'].count()\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d342df2",
   "metadata": {},
   "source": [
    "# GeoSpatial EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e108651",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load your zones data as a GeoDataFrame\n",
    "gdf_zones = gpd.read_file('path_to_your_zones_shapefile.shp')  # Adjust the path to your shapefile\n",
    "\n",
    "# If your zones data is in CSV and contains WKT or coordinates, convert it to GeoDataFrame\n",
    "# gdf_zones = gpd.GeoDataFrame(zones, geometry=gpd.points_from_xy(zones.longitude, zones.latitude))\n",
    "\n",
    "# Calculate pickup and dropoff counts per zone\n",
    "pickup_counts = df['PULocationID'].value_counts().rename_axis('LocationID').reset_index(name='pickup_count')\n",
    "dropoff_counts = df['DOLocationID'].value_counts().rename_axis('LocationID').reset_index(name='dropoff_count')\n",
    "\n",
    "# Merge these counts with the zones GeoDataFrame\n",
    "gdf_zones = gdf_zones.merge(pickup_counts, on='LocationID', how='left')\n",
    "gdf_zones = gdf_zones.merge(dropoff_counts, on='LocationID', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a635a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(20, 10))\n",
    "\n",
    "# Plot pickup counts\n",
    "gdf_zones.plot(column='pickup_count', ax=ax[0], legend=True,\n",
    "               legend_kwds={'label': \"Pickup Count by Zone\", 'orientation': \"horizontal\"})\n",
    "ax[0].set_title('Taxi Pickup Heatmap')\n",
    "ax[0].set_axis_off()\n",
    "\n",
    "# Plot dropoff counts\n",
    "gdf_zones.plot(column='dropoff_count', ax=ax[1], legend=True,\n",
    "               legend_kwds={'label': \"Dropoff Count by Zone\", 'orientation': \"horizontal\"})\n",
    "ax[1].set_title('Taxi Dropoff Heatmap')\n",
    "ax[1].set_axis_off()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528fdaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "\n",
    "# Create a map centered around an average location in NYC\n",
    "m = folium.Map(location=[40.730610, -73.935242], zoom_start=11)\n",
    "\n",
    "# Create a Choropleth map for pickups\n",
    "folium.Choropleth(\n",
    "    geo_data=gdf_zones,\n",
    "    name='choropleth',\n",
    "    data=gdf_zones,\n",
    "    columns=['LocationID', 'pickup_count'],\n",
    "    key_on='feature.properties.LocationID',\n",
    "    fill_color='YlOrRd',\n",
    "    fill_opacity=0.7,\n",
    "    line_opacity=0.2,\n",
    "    legend_name='Taxi Pickup Volume'\n",
    ").add_to(m)\n",
    "\n",
    "m.save('NYC_taxi_pickup_map.html')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b8fdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by drop-off location and calculate the average fare\n",
    "destination_pricing = df.groupby('DOLocationID')['fare_amount'].mean().sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81eda7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "# If the destination IDs are too many, you can limit the output for better visualization\n",
    "top_destinations = destination_pricing.head(20)\n",
    "sns.barplot(x=top_destinations.index, y=top_destinations.values, palette=\"coolwarm\")\n",
    "plt.title('Average Taxi Fare by Destination')\n",
    "plt.xlabel('Destination Location ID')\n",
    "plt.ylabel('Average Fare ($)')\n",
    "plt.xticks(rotation=45)  # Rotate labels for better readability if necessary\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b560bb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# Example: Assuming 'longitude' and 'latitude' are in the 'zones' dataframe\n",
    "gdf = gpd.GeoDataFrame(\n",
    "    df, geometry=gpd.points_from_xy(df.longitude, df.latitude)\n",
    ")\n",
    "\n",
    "# Join the average fare data back onto the zones GeoDataFrame\n",
    "gdf = gdf.merge(destination_pricing, on='DOLocationID')\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(1, 1, figsize=(15, 15))\n",
    "gdf.plot(column='fare_amount', ax=ax, legend=True,\n",
    "         legend_kwds={'label': \"Average Fare ($)\",\n",
    "                      'orientation': \"horizontal\"})\n",
    "plt.title('Heatmap of Average Taxi Fares by Destination')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36aea13",
   "metadata": {},
   "source": [
    "# Correlation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9abf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr = df.corr()\n",
    "\n",
    "# Generate a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr, annot=True, fmt=\".2f\", cmap='coolwarm')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300ac4c3",
   "metadata": {},
   "source": [
    "Step 2: Redundancy and Multicollinearity Check\n",
    "Identify features that are highly correlated with each other. High multicollinearity can be problematic in regression models because it can make the model's estimates very sensitive to changes in the model.\n",
    "\n",
    "Drop redundant features: If two features are highly correlated, consider dropping one.\n",
    "Principal Component Analysis (PCA): For dimensionality reduction, especially if you have a very high number of features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3620ee41",
   "metadata": {},
   "source": [
    "# Feature Importance\n",
    "Utilize machine learning algorithms like Random Forest or Gradient Boosting to identify feature importance. This will provide insights into which features have the most impact on your target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c78be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Assuming all necessary preprocessing is done\n",
    "model = RandomForestRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "feature_importances = pd.DataFrame(model.feature_importances_,\n",
    "                                   index = X_train.columns,\n",
    "                                   columns=['importance']).sort_values('importance', ascending=False)\n",
    "print(feature_importances)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1bb46e",
   "metadata": {},
   "source": [
    "Step 4: Feature Engineering Revisit\n",
    "Based on the insights from the correlation and importance analysis:\n",
    "\n",
    "Create new features: Sometimes interactions between features (e.g., multiplying or dividing two features) might have a better correlation with the target.\n",
    "Grouping: Grouping sparse categorical features based on similarity or impact on the target variable can sometimes improve model performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df548bc7",
   "metadata": {},
   "source": [
    "# Feature Selection and Model Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99214e2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9b16c09f",
   "metadata": {},
   "source": [
    "# Normalization & Standartization\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df_scaled = scaler.fit_transform(df[['fare_amount', 'trip_distance', 'trip_duration']])  # include necessary features\n",
    "scaler = MinMaxScaler()\n",
    "df_normalized = scaler.fit_transform(df[['fare_amount', 'trip_distance', 'trip_duration']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c82f83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "# Standardization\n",
    "scaler = StandardScaler()\n",
    "df_scaled = scaler.fit_transform(df[['fare_amount', 'trip_distance', 'trip_duration']])  # include necessary features\n",
    "\n",
    "# Normalization\n",
    "scaler = MinMaxScaler()\n",
    "df_normalized = scaler.fit_transform(df[['fare_amount', 'trip_distance', 'trip_duration']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8c9996",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
