{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6b04b3c",
   "metadata": {},
   "source": [
    "# Introduction to Data Preparation for NYC Taxi Trips\n",
    "\n",
    "## Objective\n",
    "This Jupyter Notebook is dedicated to the initial stage of our data analytics project—**Data Preparation**. The primary goal here is to prepare the vast New York City Taxi Trips dataset for the year 2021, ensuring it's clean, organized, and ready for in-depth analysis and modeling in subsequent stages. The dataset, which includes over 30 million records, requires meticulous handling to manage its volume and enhance its quality effectively.\n",
    "\n",
    "## Background\n",
    "The NYC Taxi Trips dataset is sourced from the NYC Open Data portal and offers a detailed snapshot of taxi activities across New York City. It records every taxi trip's core details, such as times of pickup and dropoff, trip distances, fares charged, and more. These records not only provide insights into the city’s mobility patterns but also serve as a basis for predictive modeling of fares and understanding factors influencing taxi trip dynamics.\n",
    "\n",
    "## Scope of This Notebook\n",
    "In this notebook, we will perform several key tasks to prepare the data for further analysis:\n",
    "1. **Data Loading**: Load the data from four pre-processed partitions to manage the dataset's size efficiently.\n",
    "2. **Initial Exploration**: Conduct a preliminary examination to understand the dataset's structure, missing values, anomalies, and data types.\n",
    "3. **Data Cleaning**: Address missing or incorrect values, remove duplicates, and handle any outliers or erroneous entries.\n",
    "4. **Feature Engineering**: Develop new features that are more informative for analysis and predictive modeling, such as calculating trip durations and categorizing times of day.\n",
    "5. **Data Transformation**: Standardize and normalize data as necessary to prepare for machine learning algorithms that require standardized input.\n",
    "6. **Data Reduction**: Reduce dimensionality where applicable to improve model performance and decrease computational requirements.\n",
    "\n",
    "## Tools and Libraries\n",
    "We will use Python as our main programming language, leveraging libraries such as Pandas for data manipulation, Numpy for numerical operations, and Matplotlib/Seaborn for visualization purposes.Dask for loading, memory optimization and data cleaning. These tools are chosen for their efficiency and ease of use in handling large datasets like ours.\n",
    "Utilizing Dask for Efficient Data Handling in NYC Taxi Trips Analysis\n",
    "In this Jupyter Notebook, we embark on the crucial stage of Data Preparation for the NYC Taxi Trips dataset from 2021. Given the dataset's extensive volume, encompassing over 30 million records, efficient management is paramount. Our objective is to clean, organize, and ready the dataset for detailed exploratory analysis and advanced modeling.\n",
    "Why Dask?\n",
    "Handling such a massive dataset requires tools that extend beyond traditional data processing capabilities. Here, we introduce Dask, a flexible parallel computing library designed to integrate seamlessly with Pandas and Numpy. Dask enables us to work with large datasets that don't fit entirely in memory by breaking them down into manageable chunks, allowing for parallel computation on a single machine or across a cluster.\n",
    "\n",
    "Dask's Role in Our Project\n",
    "Dask will be utilized primarily for:\n",
    "\n",
    "Efficient Data Loading: To handle data across four pre-processed partitions without overwhelming system memory.\n",
    "Data Manipulation and Transformation: Including concatenation of multiple dataframes, handling missing values, outlier correction, and feature engineering, all performed in a way that optimizes memory use and computational speed.\n",
    "By leveraging Dask, we can maintain the integrity of our data manipulation processes while ensuring that our operations are scalable and efficient. This approach not only facilitates faster data processing but also enhances our capability to manage data intricacies due to the size of the dataset.\n",
    "\n",
    "\n",
    "## Conclusion\n",
    "By the end of this notebook, the dataset will be transformed into a clean, comprehensive format suitable for detailed exploratory data analysis and machine learning tasks in the following stages of this project. The meticulous preparation we perform here is crucial for ensuring the accuracy and reliability of our later analyses and predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda3fb64",
   "metadata": {},
   "source": [
    "# 1. Data Loading \n",
    "In this section we import all necessary libraries and tools used for data preprocessing as mentioned above. \n",
    "\n",
    "We will first load the pickle partitioned datasets, we then will analyze each partitioned dataset separately to understand data structures and create one combined dask file for further cleaning and data preparation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8d38c2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import dask.dataframe as dd\n",
    "\n",
    "\n",
    "# Set some options for displaying the data tables nicely\n",
    "pd.set_option('display.max_columns', None)  # Show all columns of DataFrames\n",
    "pd.set_option('display.width', 1000)        # Ensure the display is wide enough to view all DataFrame columns\n",
    "pd.set_option('display.float_format', '{:,.2f}'.format)  # Format floats for easier reading\n",
    "\n",
    "# Setting the style for seaborn plots\n",
    "sns.set(style=\"whitegrid\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aab060a",
   "metadata": {},
   "source": [
    "Pickle is a Python-specific binary serialization method used to save and load Python objects directly, preserving their data types and structure. In our NYC Taxi Trips data analytics project, we chose Pickle for its efficiency and ease of use, especially given the large volume of data involved. It enables fast loading and saving of complex Pandas DataFrames, significantly speeding up our workflow by avoiding repeated preprocessing. Although Pickle should be used cautiously due to security risks when dealing with untrusted data sources, it is ideal for our controlled environment where these concerns are mitigated.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9dc76e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded from Pickle files.\n"
     ]
    }
   ],
   "source": [
    "# Define data types for data consistency\n",
    "dtypes = {\n",
    "    'VendorID': 'category',\n",
    "    'tpep_pickup_datetime': 'str',\n",
    "    'tpep_dropoff_datetime': 'str',\n",
    "    'passenger_count': 'float64',\n",
    "    'trip_distance': 'float64',\n",
    "    'RatecodeID': 'category',\n",
    "    'store_and_fwd_flag': 'category',\n",
    "    'PULocationID': 'category',\n",
    "    'DOLocationID': 'category',\n",
    "    'payment_type': 'category',\n",
    "    'fare_amount': 'float64',\n",
    "    'extra': 'float64',\n",
    "    'mta_tax': 'float64',\n",
    "    'tip_amount': 'float64',\n",
    "    'tolls_amount': 'float64',\n",
    "    'improvement_surcharge': 'float64',\n",
    "    'total_amount': 'float64',\n",
    "    'congestion_surcharge': 'float64'\n",
    "}\n",
    "\n",
    "# Load data from CSV files only if Pickle files do not exist or when processing for the first time\n",
    "try:\n",
    "    df1 = pd.read_pickle(\"/Users/md/Desktop/python_project/df1.pkl\")\n",
    "    df2 = pd.read_pickle(\"/Users/md/Desktop/python_project/df2.pkl\")\n",
    "    df3 = pd.read_pickle(\"/Users/md/Desktop/python_project/df3.pkl\")\n",
    "    df4 = pd.read_pickle(\"/Users/md/Desktop/python_project/df4.pkl\")\n",
    "    print(\"Data loaded from Pickle files.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Pickle files not found. Loading data from CSV files and saving as Pickle.\")\n",
    "    df1 = pd.read_csv(\"2021_TLC_0.csv\", dtype=dtypes)\n",
    "    df2 = pd.read_csv(\"2021_TLC_1.csv\", dtype=dtypes)\n",
    "    df3 = pd.read_csv(\"2021_TLC_2.csv\", dtype=dtypes)\n",
    "    df4 = pd.read_csv(\"2021_TLC_3.csv\", dtype=dtypes)\n",
    "    # Save DataFrames to Pickle for future use\n",
    "    df1.to_pickle(\"/Users/md/Desktop/python_project/df1.pkl\")\n",
    "    df2.to_pickle(\"/Users/md/Desktop/python_project/df2.pkl\")\n",
    "    df3.to_pickle(\"/Users/md/Desktop/python_project/df3.pkl\")\n",
    "    df4.to_pickle(\"/Users/md/Desktop/python_project/df4.pkl\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "52803b8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VendorID</th>\n",
       "      <th>tpep_pickup_datetime</th>\n",
       "      <th>tpep_dropoff_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>RatecodeID</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>extra</th>\n",
       "      <th>mta_tax</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>tolls_amount</th>\n",
       "      <th>improvement_surcharge</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>congestion_surcharge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>10/28/2021 03:35:00 PM</td>\n",
       "      <td>10/28/2021 03:44:01 PM</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>170.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.30</td>\n",
       "      <td>12.30</td>\n",
       "      <td>2.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>10/28/2021 03:45:48 PM</td>\n",
       "      <td>10/28/2021 04:04:51 PM</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.45</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>79.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.30</td>\n",
       "      <td>18.96</td>\n",
       "      <td>2.50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  VendorID    tpep_pickup_datetime   tpep_dropoff_datetime  passenger_count  trip_distance RatecodeID store_and_fwd_flag PULocationID DOLocationID payment_type  fare_amount  extra  mta_tax  tip_amount  tolls_amount  improvement_surcharge  total_amount  congestion_surcharge\n",
       "0      2.0  10/28/2021 03:35:00 PM  10/28/2021 03:44:01 PM             1.00           1.80        1.0                  N        170.0         79.0          1.0         8.00   0.00     0.50        1.00          0.00                   0.30         12.30                  2.50\n",
       "1      2.0  10/28/2021 03:45:48 PM  10/28/2021 04:04:51 PM             1.00           1.45        1.0                  N         79.0        170.0          1.0        12.50   0.00     0.50        3.16          0.00                   0.30         18.96                  2.50"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabcf395",
   "metadata": {},
   "source": [
    "In the section above we loaded all 4 partitioned datasets for analysis defined data types based on the documentation for the dataset. We then loaded the Dataframes into pickle format for faster processing and for the purpose of tracking and maintaining consistency throughout rest of the notebook without the need to load large CSV files into df after every launch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcae14a",
   "metadata": {},
   "source": [
    "#### Column Descriptions Per Data Dictonary and The Datasets\n",
    "* 1 vendor ID- A code indicating the TPEP provider that provided the record, 1=Creative Mobile Technologies, LLC; 2=VeriFone Inc.\n",
    "\n",
    "* tpep_pickup_datetime- The date and time when the meter was engaged.\n",
    "* tpep_dropoff_datetime- The date and time when the meter was disengaged. \n",
    "* passenger_count\tThe number of passengers in the vehicle.\n",
    "* trip_distance\tThe elapsed trip distance in miles reported by the taximeter\n",
    "* RatecodeID The final rate code in effect at the end of the trip.\t\n",
    "    **1= Standard rate\n",
    "    **2=JFK\n",
    "    **3=Newark\n",
    "    **4=Nassau or Westchester\n",
    "    **5=Negotiated fare\n",
    "    **6=Group ride\n",
    "    **99 = Null/unknown\n",
    "* store_and_fwd_flag\tThis flag indicates whether the trip record was held in vehicle memory before sending to the vendor, aka “store and forward,” because the vehicle did not have a connection to the server.\t\"Y= store and forward trip N= not a store and forward trip\"\n",
    "\n",
    "* PULocationID\tTLC Taxi Zone in which the taximeter was engaged\n",
    "* DOLocationID\tTLC Taxi Zone in which the taximeter was disengaged\n",
    "\n",
    "* payment_type\tA numeric code signifying how the passenger paid for the trip.\t\"0= Flex Fare trip\n",
    "     **1= Credit card\n",
    "     **2= Cash\n",
    "     **3= No charge\n",
    "     **4= Dispute\n",
    "     **5= Unknown\n",
    "     **6= Voided trip\n",
    "* fare_amount\tThe time-and-distance fare calculated by the meter. \n",
    "\n",
    "* extra\tMiscellaneous extras and surcharges.\n",
    "* mta_tax\tTax that is automatically triggered based on the metered rate in use.\n",
    "* tip_amount\tTip amount – This field is automatically populated for credit card tips. Cash tips are not included.\n",
    "* tolls_amount\tTotal amount of all tolls paid in trip.\n",
    "* improvement_surcharge\tImprovement surcharge assessed trips at the flag drop. The improvement surcharge began being levied in 2015.\n",
    "* total_amount\tThe total amount charged to passengers. Does not include cash tips.\n",
    "* congestion_surcharge\tTotal amount collected in trip for NYS congestion surcharge.\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d708ed2e",
   "metadata": {},
   "source": [
    "### 1.1 Initial Exploration Of Partitioned Datasets\n",
    "\n",
    "After loading the data, it's crucial to understand its structure and understanding how we can workk with it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa0aa9a",
   "metadata": {},
   "source": [
    "#### 1.1.1 Partition 1 Initial Exploration\n",
    "\n",
    "The DataFrame `df1` provides a comprehensive overview of the 2021 NYC Taxi Trips dataset, which contains approximately 7.92 million entries across 18 attributes. The data spans categorical variables like `VendorID` and `RatecodeID`, datetime variables for pickup and dropoff, and numerical values for passenger counts, trip distances, and fare amounts. Notably, the data reveals a central tendency where most trips average 1.41 passengers with trip distances of 5.02 miles, and an average fare of $12.36. However, the presence of significant outliers in fare and distance, and anomalies such as negative values, suggest data entry errors or rare long-distance trips. The dataset shows no missing values in monetary or distance-related fields but highlights gaps in categorical data, which could impact demographic analysis. For effective data utilization, steps will include cleaning to address outliers, filling missing values based on their distribution and impact, and enhancing the dataset through feature engineering like calculating trip durations and categorizing times of day. These measures will prepare the dataset for further comprehensive analysis and modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d5c3e834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  VendorID tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  trip_distance RatecodeID store_and_fwd_flag PULocationID DOLocationID payment_type  fare_amount  extra  mta_tax  tip_amount  tolls_amount  improvement_surcharge  total_amount  congestion_surcharge\n",
      "0      1.0  2021-01-01 00:30:10   2021-01-01 00:36:12             1.00           2.10        1.0                  N        142.0         43.0          2.0         8.00   3.00     0.50        0.00          0.00                   0.30         11.80                  2.50\n",
      "1      1.0  2021-01-01 00:51:20   2021-01-01 00:52:19             1.00           0.20        1.0                  N        238.0        151.0          2.0         3.00   0.50     0.50        0.00          0.00                   0.30          4.30                  0.00\n",
      "2      1.0  2021-01-01 00:43:30   2021-01-01 01:11:06             1.00          14.70        1.0                  N        132.0        165.0          1.0        42.00   0.50     0.50        8.65          0.00                   0.30         51.95                  0.00\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7919804 entries, 0 to 7919803\n",
      "Data columns (total 18 columns):\n",
      " #   Column                 Dtype         \n",
      "---  ------                 -----         \n",
      " 0   VendorID               category      \n",
      " 1   tpep_pickup_datetime   datetime64[ns]\n",
      " 2   tpep_dropoff_datetime  datetime64[ns]\n",
      " 3   passenger_count        float64       \n",
      " 4   trip_distance          float64       \n",
      " 5   RatecodeID             category      \n",
      " 6   store_and_fwd_flag     category      \n",
      " 7   PULocationID           category      \n",
      " 8   DOLocationID           category      \n",
      " 9   payment_type           category      \n",
      " 10  fare_amount            float64       \n",
      " 11  extra                  float64       \n",
      " 12  mta_tax                float64       \n",
      " 13  tip_amount             float64       \n",
      " 14  tolls_amount           float64       \n",
      " 15  improvement_surcharge  float64       \n",
      " 16  total_amount           float64       \n",
      " 17  congestion_surcharge   float64       \n",
      "dtypes: category(6), datetime64[ns](2), float64(10)\n",
      "memory usage: 785.5 MB\n",
      "None\n",
      "       passenger_count  trip_distance  fare_amount        extra      mta_tax   tip_amount  tolls_amount  improvement_surcharge  total_amount  congestion_surcharge\n",
      "count     7,467,266.00   7,919,804.00 7,919,804.00 7,919,804.00 7,919,804.00 7,919,804.00  7,919,804.00           7,919,804.00  7,919,804.00          7,919,804.00\n",
      "mean              1.41           5.02        12.36         1.05         0.49         2.00          0.25                   0.30         17.96                  2.17\n",
      "std               1.03         497.23       199.96         1.26         0.07         2.45          1.43                   0.04        200.07                  0.87\n",
      "min               0.00           0.00      -643.50        -5.50        -0.50      -333.32        -38.02                  -0.30       -647.80                 -2.50\n",
      "25%               1.00           1.01         6.50         0.00         0.50         0.00          0.00                   0.30         11.16                  2.50\n",
      "50%               1.00           1.71         9.00         0.50         0.50         1.86          0.00                   0.30         14.16                  2.50\n",
      "75%               1.00           3.04        13.50         2.50         0.50         2.76          0.00                   0.30         19.56                  2.50\n",
      "max               9.00     280,567.84   398,466.38        90.06         3.85     1,140.44        811.75                   0.30    398,469.20                  3.00\n"
     ]
    }
   ],
   "source": [
    "# Display the first few rows of the first DataFrame\n",
    "print(df1.head(3))\n",
    "\n",
    "# Display summary information about DataFrame\n",
    "print(df1.info())\n",
    "\n",
    "# Basic statistical details\n",
    "print(df1.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d11860d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in each column:\n",
      " VendorID                 452538\n",
      "tpep_pickup_datetime          0\n",
      "tpep_dropoff_datetime         0\n",
      "passenger_count          452538\n",
      "trip_distance                 0\n",
      "RatecodeID               452538\n",
      "store_and_fwd_flag       452538\n",
      "PULocationID                  0\n",
      "DOLocationID                  0\n",
      "payment_type             452538\n",
      "fare_amount                   0\n",
      "extra                         0\n",
      "mta_tax                       0\n",
      "tip_amount                    0\n",
      "tolls_amount                  0\n",
      "improvement_surcharge         0\n",
      "total_amount                  0\n",
      "congestion_surcharge          0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Count missing values in each column\n",
    "missing_values1 = df1.isnull().sum()\n",
    "print(\"Missing values in each column:\\n\", missing_values1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc440887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data types of each column:\n",
      " VendorID                       category\n",
      "tpep_pickup_datetime     datetime64[ns]\n",
      "tpep_dropoff_datetime    datetime64[ns]\n",
      "passenger_count                 float64\n",
      "trip_distance                   float64\n",
      "RatecodeID                     category\n",
      "store_and_fwd_flag             category\n",
      "PULocationID                   category\n",
      "DOLocationID                   category\n",
      "payment_type                   category\n",
      "fare_amount                     float64\n",
      "extra                           float64\n",
      "mta_tax                         float64\n",
      "tip_amount                      float64\n",
      "tolls_amount                    float64\n",
      "improvement_surcharge           float64\n",
      "total_amount                    float64\n",
      "congestion_surcharge            float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"Data types of each column:\\n\", df1.dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e10da00",
   "metadata": {},
   "source": [
    "#### 1.1.2 Partition 2 Initial Exploration\n",
    "\n",
    "The DataFrame from partition 2 of the NYC Taxi Trips dataset offers a detailed look at approximately 7.89 million taxi trips, with a wide range of data points from 18 different attributes. This partition includes both categorical variables like `VendorID` and `RatecodeID`, and continuous variables such as `trip_distance` and `fare_amount`. Notably, the dataset records an average trip distance of 7.91 miles and an average fare amount of $13.46, although extremes in data (such as a maximum trip distance of over 332,541 miles) suggest the presence of significant outliers or errors. There is a noticeable issue with missing data in several categorical fields, including `VendorID` and `payment_type`, totaling 381,490 missing entries for several categories, which could affect the completeness and reliability of any demographic or fare-based analysis. The data types are appropriately assigned, facilitating efficient data handling and analysis. The next steps include cleaning the dataset to address outliers, filling in missing values where possible, and potentially simplifying the dataset through feature engineering to better focus on the most impactful variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20a4a798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  VendorID    tpep_pickup_datetime   tpep_dropoff_datetime  passenger_count  trip_distance RatecodeID store_and_fwd_flag PULocationID DOLocationID payment_type  fare_amount  extra  mta_tax  tip_amount  tolls_amount  improvement_surcharge  total_amount  congestion_surcharge\n",
      "0      2.0  05/15/2021 10:35:13 AM  05/15/2021 11:08:19 AM             5.00          17.89        2.0                  N        132.0        224.0          1.0        52.00   0.00     0.50        8.00          6.55                   0.30         69.85                  2.50\n",
      "1      2.0  05/15/2021 10:13:57 AM  05/15/2021 10:19:57 AM             2.00           1.11        1.0                  N        114.0        249.0          1.0         6.50   0.00     0.50        1.96          0.00                   0.30         11.76                  2.50\n",
      "2      2.0  05/15/2021 10:26:33 AM  05/15/2021 10:43:56 AM             2.00           4.44        1.0                  N        100.0         12.0          1.0        16.50   0.00     0.50        3.96          0.00                   0.30         23.76                  2.50\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7887665 entries, 0 to 7887664\n",
      "Data columns (total 18 columns):\n",
      " #   Column                 Dtype   \n",
      "---  ------                 -----   \n",
      " 0   VendorID               category\n",
      " 1   tpep_pickup_datetime   object  \n",
      " 2   tpep_dropoff_datetime  object  \n",
      " 3   passenger_count        float64 \n",
      " 4   trip_distance          float64 \n",
      " 5   RatecodeID             category\n",
      " 6   store_and_fwd_flag     category\n",
      " 7   PULocationID           category\n",
      " 8   DOLocationID           category\n",
      " 9   payment_type           category\n",
      " 10  fare_amount            float64 \n",
      " 11  extra                  float64 \n",
      " 12  mta_tax                float64 \n",
      " 13  tip_amount             float64 \n",
      " 14  tolls_amount           float64 \n",
      " 15  improvement_surcharge  float64 \n",
      " 16  total_amount           float64 \n",
      " 17  congestion_surcharge   float64 \n",
      "dtypes: category(6), float64(10), object(2)\n",
      "memory usage: 782.3+ MB\n",
      "None\n",
      "       passenger_count  trip_distance  fare_amount        extra      mta_tax   tip_amount  tolls_amount  improvement_surcharge  total_amount  congestion_surcharge\n",
      "count     7,506,175.00   7,887,665.00 7,887,665.00 7,887,665.00 7,887,665.00 7,887,665.00  7,887,665.00           7,887,665.00  7,887,665.00          7,887,665.00\n",
      "mean              1.45           7.91        13.46         1.04         0.49         2.32          0.39                   0.30         19.63                  2.24\n",
      "std               1.06         780.95        12.57         1.25         0.08         2.79          1.90                   0.04         15.21                  0.79\n",
      "min               0.00           0.00      -624.00        -5.50        -0.50      -130.10        -34.05                  -0.30       -627.80                 -2.50\n",
      "25%               1.00           1.10         6.50         0.00         0.50         0.00          0.00                   0.30         11.76                  2.50\n",
      "50%               1.00           1.85         9.50         0.50         0.50         2.00          0.00                   0.30         15.20                  2.50\n",
      "75%               2.00           3.36        15.00         2.50         0.50         3.00          0.00                   0.30         21.23                  2.50\n",
      "max             112.00     332,541.19     1,320.00        45.50         4.55       449.21        956.55                   0.30      1,320.80                  2.75\n"
     ]
    }
   ],
   "source": [
    "# Display the first few rows of the first DataFrame\n",
    "print(df2.head(3))\n",
    "\n",
    "# Display summary information about DataFrame\n",
    "print(df2.info())\n",
    "\n",
    "# Basic statistical details\n",
    "print(df2.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee21f0fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in each column:\n",
      " VendorID                 381490\n",
      "tpep_pickup_datetime          0\n",
      "tpep_dropoff_datetime         0\n",
      "passenger_count          381490\n",
      "trip_distance                 0\n",
      "RatecodeID               381490\n",
      "store_and_fwd_flag       381490\n",
      "PULocationID                  0\n",
      "DOLocationID                  0\n",
      "payment_type             381490\n",
      "fare_amount                   0\n",
      "extra                         0\n",
      "mta_tax                       0\n",
      "tip_amount                    0\n",
      "tolls_amount                  0\n",
      "improvement_surcharge         0\n",
      "total_amount                  0\n",
      "congestion_surcharge          0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Count missing values in each column\n",
    "missing_values2 = df2.isnull().sum()\n",
    "print(\"Missing values in each column:\\n\", missing_values2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed5c9781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data types of each column:\n",
      " VendorID                 category\n",
      "tpep_pickup_datetime       object\n",
      "tpep_dropoff_datetime      object\n",
      "passenger_count           float64\n",
      "trip_distance             float64\n",
      "RatecodeID               category\n",
      "store_and_fwd_flag       category\n",
      "PULocationID             category\n",
      "DOLocationID             category\n",
      "payment_type             category\n",
      "fare_amount               float64\n",
      "extra                     float64\n",
      "mta_tax                   float64\n",
      "tip_amount                float64\n",
      "tolls_amount              float64\n",
      "improvement_surcharge     float64\n",
      "total_amount              float64\n",
      "congestion_surcharge      float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"Data types of each column:\\n\", df2.dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3e867d",
   "metadata": {},
   "source": [
    "#### 1.1.3 Partition 3 Initial Exploration\n",
    "\n",
    "The third partition of the dataset consists of approximately 7.87 million entries, each detailing aspects of taxi trips such as passenger count, trip distance, and fare amounts. Key metrics show an average trip distance of 6.02 miles and an average fare amount of $13.99. The data indicates a wide range of fares and distances, including outliers with extreme values that highlight potential errors or extraordinary trip scenarios. A noticeable portion of data is missing for several categorical variables like RatecodeID and congestion_surcharge, amounting to 278,444 missing entries which could influence analysis outcomes. The dataset is structured effectively for analysis, with categorical data stored efficiently and numerical data in a format conducive to statistical operations. Future steps involve rigorous data cleaning to rectify anomalies and address missing values, alongside exploring data relationships and potential feature engineering to enrich the dataset’s analytical value.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "986e34a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  VendorID    tpep_pickup_datetime   tpep_dropoff_datetime  passenger_count  trip_distance RatecodeID store_and_fwd_flag PULocationID DOLocationID payment_type  fare_amount  extra  mta_tax  tip_amount  tolls_amount  improvement_surcharge  total_amount  congestion_surcharge\n",
      "0      1.0  08/10/2021 04:22:40 PM  08/10/2021 04:40:02 PM             1.00           2.50        1.0                  N        170.0        236.0          1.0        13.00   3.50     0.50        3.45          0.00                   0.30         20.75                  2.50\n",
      "1      1.0  08/10/2021 04:11:09 PM  08/10/2021 04:19:01 PM             0.00           1.00        1.0                  N        107.0         90.0          2.0         7.00   3.50     0.50        0.00          0.00                   0.30         11.30                  2.50\n",
      "2      1.0  08/10/2021 04:20:56 PM  08/10/2021 04:35:54 PM             0.00           2.40        1.0                  N         68.0        142.0          1.0        11.50   3.50     0.50        3.15          0.00                   0.30         18.95                  2.50\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7873284 entries, 0 to 7873283\n",
      "Data columns (total 18 columns):\n",
      " #   Column                 Dtype   \n",
      "---  ------                 -----   \n",
      " 0   VendorID               category\n",
      " 1   tpep_pickup_datetime   object  \n",
      " 2   tpep_dropoff_datetime  object  \n",
      " 3   passenger_count        float64 \n",
      " 4   trip_distance          float64 \n",
      " 5   RatecodeID             category\n",
      " 6   store_and_fwd_flag     category\n",
      " 7   PULocationID           category\n",
      " 8   DOLocationID           category\n",
      " 9   payment_type           category\n",
      " 10  fare_amount            float64 \n",
      " 11  extra                  float64 \n",
      " 12  mta_tax                float64 \n",
      " 13  tip_amount             float64 \n",
      " 14  tolls_amount           float64 \n",
      " 15  improvement_surcharge  float64 \n",
      " 16  total_amount           float64 \n",
      " 17  congestion_surcharge   float64 \n",
      "dtypes: category(6), float64(10), object(2)\n",
      "memory usage: 780.9+ MB\n",
      "None\n",
      "       passenger_count  trip_distance  fare_amount        extra      mta_tax   tip_amount  tolls_amount  improvement_surcharge  total_amount  congestion_surcharge\n",
      "count     7,594,840.00   7,873,284.00 7,873,284.00 7,873,284.00 7,873,284.00 7,873,284.00  7,873,284.00           7,873,284.00  7,873,284.00          7,594,840.00\n",
      "mean              1.42           6.02        13.99         1.05         0.49         2.48          0.43                   0.30         20.39                  2.30\n",
      "std               1.03         579.71       291.91         1.25         0.08         2.93          1.89                   0.04        292.05                  0.71\n",
      "min               0.00           0.00      -758.00        -5.50        -0.50      -200.00        -88.75                  -0.30       -951.00                 -2.50\n",
      "25%               1.00           1.10         7.00         0.00         0.50         0.00          0.00                   0.30         11.80                  2.50\n",
      "50%               1.00           1.88        10.00         0.50         0.50         2.05          0.00                   0.30         15.36                  2.50\n",
      "75%               1.00           3.40        15.50         2.50         0.50         3.16          0.00                   0.30         21.80                  2.50\n",
      "max               9.00     317,182.45   818,283.44        41.07         4.25       500.00        911.30                   0.30    818,286.74                  2.75\n"
     ]
    }
   ],
   "source": [
    "# Display the first few rows of the first DataFrame\n",
    "print(df3.head(3))\n",
    "\n",
    "# Display summary information about DataFrame\n",
    "print(df3.info())\n",
    "\n",
    "# Basic statistical details\n",
    "print(df3.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "32e916d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in each column:\n",
      " VendorID                      0\n",
      "tpep_pickup_datetime          0\n",
      "tpep_dropoff_datetime         0\n",
      "passenger_count          278444\n",
      "trip_distance                 0\n",
      "RatecodeID               278444\n",
      "store_and_fwd_flag       278444\n",
      "PULocationID                  0\n",
      "DOLocationID                  0\n",
      "payment_type                  0\n",
      "fare_amount                   0\n",
      "extra                         0\n",
      "mta_tax                       0\n",
      "tip_amount                    0\n",
      "tolls_amount                  0\n",
      "improvement_surcharge         0\n",
      "total_amount                  0\n",
      "congestion_surcharge     278444\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Count missing values in each column\n",
    "missing_values3 = df3.isnull().sum()\n",
    "print(\"Missing values in each column:\\n\", missing_values3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60317843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data types of each column:\n",
      " VendorID                 category\n",
      "tpep_pickup_datetime       object\n",
      "tpep_dropoff_datetime      object\n",
      "passenger_count           float64\n",
      "trip_distance             float64\n",
      "RatecodeID               category\n",
      "store_and_fwd_flag       category\n",
      "PULocationID             category\n",
      "DOLocationID             category\n",
      "payment_type             category\n",
      "fare_amount               float64\n",
      "extra                     float64\n",
      "mta_tax                   float64\n",
      "tip_amount                float64\n",
      "tolls_amount              float64\n",
      "improvement_surcharge     float64\n",
      "total_amount              float64\n",
      "congestion_surcharge      float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"Data types of each column:\\n\", df3.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6605f0",
   "metadata": {},
   "source": [
    "#### 1.1.4 Partition 4 Initial Exploration\n",
    "\n",
    "The fourth partition mirrors the structure and detail of the earlier partitions, containing approximately 7.22 million records. It reports an average trip distance of 8.92 miles and an average fare amount of $14.33, with extremes in both measurements suggesting notable outliers or data integrity issues. Missing data impacts 366,223 records for categories such as RatecodeID and store_and_fwd_flag, posing challenges for complete demographic analysis. The data types are optimally designated to support varied analytical processes, from time series evaluations to categorical analyses. The next actions for this partition include comprehensive data cleaning to handle outliers and fill data gaps, further exploratory analysis to decipher underlying patterns, and the potential development of new features to better capture the dynamics of taxi trip economics and behaviors in NYC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4b8315c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  VendorID    tpep_pickup_datetime   tpep_dropoff_datetime  passenger_count  trip_distance RatecodeID store_and_fwd_flag PULocationID DOLocationID payment_type  fare_amount  extra  mta_tax  tip_amount  tolls_amount  improvement_surcharge  total_amount  congestion_surcharge\n",
      "0      2.0  10/28/2021 03:35:00 PM  10/28/2021 03:44:01 PM             1.00           1.80        1.0                  N        170.0         79.0          1.0         8.00   0.00     0.50        1.00          0.00                   0.30         12.30                  2.50\n",
      "1      2.0  10/28/2021 03:45:48 PM  10/28/2021 04:04:51 PM             1.00           1.45        1.0                  N         79.0        170.0          1.0        12.50   0.00     0.50        3.16          0.00                   0.30         18.96                  2.50\n",
      "2      1.0  10/28/2021 03:08:00 PM  10/28/2021 03:26:20 PM             2.00           1.70        1.0                  N        141.0        142.0          1.0        12.00   2.50     0.50        2.00          0.00                   0.30         17.30                  2.50\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7223319 entries, 0 to 7223318\n",
      "Data columns (total 18 columns):\n",
      " #   Column                 Dtype   \n",
      "---  ------                 -----   \n",
      " 0   VendorID               category\n",
      " 1   tpep_pickup_datetime   object  \n",
      " 2   tpep_dropoff_datetime  object  \n",
      " 3   passenger_count        float64 \n",
      " 4   trip_distance          float64 \n",
      " 5   RatecodeID             category\n",
      " 6   store_and_fwd_flag     category\n",
      " 7   PULocationID           category\n",
      " 8   DOLocationID           category\n",
      " 9   payment_type           category\n",
      " 10  fare_amount            float64 \n",
      " 11  extra                  float64 \n",
      " 12  mta_tax                float64 \n",
      " 13  tip_amount             float64 \n",
      " 14  tolls_amount           float64 \n",
      " 15  improvement_surcharge  float64 \n",
      " 16  total_amount           float64 \n",
      " 17  congestion_surcharge   float64 \n",
      "dtypes: category(6), float64(10), object(2)\n",
      "memory usage: 716.4+ MB\n",
      "None\n",
      "       passenger_count  trip_distance  fare_amount        extra      mta_tax   tip_amount  tolls_amount  improvement_surcharge  total_amount  congestion_surcharge\n",
      "count     6,857,096.00   7,223,319.00 7,223,319.00 7,223,319.00 7,223,319.00 7,223,319.00  7,223,319.00           7,223,319.00  7,223,319.00          6,857,096.00\n",
      "mean              1.43           8.92        14.33         1.05         0.49         2.59          0.50                   0.30         20.91                  2.31\n",
      "std               1.01         885.08        13.31         1.26         0.08         3.10          1.98                   0.04         16.49                  0.70\n",
      "min               0.00           0.00      -700.00        -4.50        -0.55       -98.00        -51.00                  -0.30       -702.80                 -2.50\n",
      "25%               1.00           1.10         7.00         0.00         0.50         0.00          0.00                   0.30         12.25                  2.50\n",
      "50%               1.00           1.86        10.00         0.50         0.50         2.06          0.00                   0.30         15.95                  2.50\n",
      "75%               2.00           3.46        16.00         2.50         0.50         3.27          0.00                   0.30         22.55                  2.50\n",
      "max               9.00     351,613.36     3,009.00        30.50        38.80       999.99        800.07                   0.30      3,012.30                  2.50\n"
     ]
    }
   ],
   "source": [
    "# Display the first few rows of the first DataFrame\n",
    "print(df4.head(3))\n",
    "\n",
    "# Display summary information about DataFrame\n",
    "print(df4.info())\n",
    "\n",
    "# Basic statistical details\n",
    "print(df4.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "02e7e011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in each column:\n",
      " VendorID                      0\n",
      "tpep_pickup_datetime          0\n",
      "tpep_dropoff_datetime         0\n",
      "passenger_count          366223\n",
      "trip_distance                 0\n",
      "RatecodeID               366223\n",
      "store_and_fwd_flag       366223\n",
      "PULocationID                  0\n",
      "DOLocationID                  0\n",
      "payment_type                  0\n",
      "fare_amount                   0\n",
      "extra                         0\n",
      "mta_tax                       0\n",
      "tip_amount                    0\n",
      "tolls_amount                  0\n",
      "improvement_surcharge         0\n",
      "total_amount                  0\n",
      "congestion_surcharge     366223\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Count missing values in each column\n",
    "missing_values4 = df4.isnull().sum()\n",
    "print(\"Missing values in each column:\\n\", missing_values4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "521a3b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data types of each column:\n",
      " VendorID                 category\n",
      "tpep_pickup_datetime       object\n",
      "tpep_dropoff_datetime      object\n",
      "passenger_count           float64\n",
      "trip_distance             float64\n",
      "RatecodeID               category\n",
      "store_and_fwd_flag       category\n",
      "PULocationID             category\n",
      "DOLocationID             category\n",
      "payment_type             category\n",
      "fare_amount               float64\n",
      "extra                     float64\n",
      "mta_tax                   float64\n",
      "tip_amount                float64\n",
      "tolls_amount              float64\n",
      "improvement_surcharge     float64\n",
      "total_amount              float64\n",
      "congestion_surcharge      float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"Data types of each column:\\n\", df4.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b854c5b",
   "metadata": {},
   "source": [
    "### 1.2 Integrating Dask for Scalable Data Management\n",
    "\n",
    "n this section of the Jupyter Notebook, we focus on integrating Dask to enhance our data handling capabilities for the large NYC Taxi Trips dataset. The section begins with defining a function, load_pickle_as_dask, which efficiently loads data stored in pickle format using Pandas and then converts it into a Dask DataFrame. This conversion is crucial as it allows us to manage the dataset in partitions, harnessing Dask’s ability to handle data that exceeds system memory limitations. We apply this function to load four separate data partitions and then utilize Dask’s concat function to merge them into a single DataFrame. This approach not only ensures efficient memory management but also sets the stage for more complex data manipulations and analysis that require handling large volumes of data seamlessly. This methodical preparation of our data underscores our commitment to maintaining high performance and scalability throughout the analysis lifecycle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7e92d7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load a pickle file and convert to a Dask DataFrame\n",
    "def load_pickle_as_dask(filepath):\n",
    "    # Read the pickle file using Pandas\n",
    "    pdf = pd.read_pickle(filepath)\n",
    "    # Convert the Pandas DataFrame to a Dask DataFrame\n",
    "    return dd.from_pandas(pdf, npartitions=10)\n",
    "\n",
    "# Load data using the function defined above\n",
    "df1 = load_pickle_as_dask(\"/Users/md/Desktop/python_project/df1.pkl\")\n",
    "df2 = load_pickle_as_dask(\"/Users/md/Desktop/python_project/df2.pkl\")\n",
    "df3 = load_pickle_as_dask(\"/Users/md/Desktop/python_project/df3.pkl\")\n",
    "df4 = load_pickle_as_dask(\"/Users/md/Desktop/python_project/df4.pkl\")\n",
    "\n",
    "# Combine dataframes using Dask\n",
    "df_combined = dd.concat([df1, df2, df3, df4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05049472",
   "metadata": {},
   "source": [
    "# 2. Initial Exploration Of Combined Dataset\n",
    "\n",
    "To ensure data integrity and to understand how our combined data looks now we will look at the concatanated combined df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "34f18987",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VendorID</th>\n",
       "      <th>tpep_pickup_datetime</th>\n",
       "      <th>tpep_dropoff_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>RatecodeID</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>extra</th>\n",
       "      <th>mta_tax</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>tolls_amount</th>\n",
       "      <th>improvement_surcharge</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>congestion_surcharge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2021-01-01 00:30:10</td>\n",
       "      <td>2021-01-01 00:36:12</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>142.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.30</td>\n",
       "      <td>11.80</td>\n",
       "      <td>2.50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  VendorID tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  trip_distance RatecodeID store_and_fwd_flag PULocationID DOLocationID payment_type  fare_amount  extra  mta_tax  tip_amount  tolls_amount  improvement_surcharge  total_amount  congestion_surcharge\n",
       "0      1.0  2021-01-01 00:30:10   2021-01-01 00:36:12             1.00           2.10        1.0                  N        142.0         43.0          2.0         8.00   3.00     0.50        0.00          0.00                   0.30         11.80                  2.50"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e70ddc78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows in df1: 7919804\n",
      "Total rows in df2: 7887665\n",
      "Total rows in df3: 7873284\n",
      "Total rows in df4: 7223319\n",
      "Total rows in all partitions: 30904072\n",
      "Total rows in combined DataFrame: 30904072\n",
      "The sum of rows from all partitions matches the total rows in the combined DataFrame.\n"
     ]
    }
   ],
   "source": [
    "#to ensure that our new combined df has all the data from partitoned files we use code below\n",
    "# Calculate the total number of rows in each partition\n",
    "num_rows_df1 = df1.shape[0].compute()\n",
    "num_rows_df2 = df2.shape[0].compute()\n",
    "num_rows_df3 = df3.shape[0].compute()\n",
    "num_rows_df4 = df4.shape[0].compute()\n",
    "\n",
    "# Sum the rows of all individual partitions\n",
    "total_rows_partitions = num_rows_df1 + num_rows_df2 + num_rows_df3 + num_rows_df4\n",
    "\n",
    "# Calculate the total number of rows in the combined DataFrame\n",
    "total_rows_combined = df_combined.shape[0].compute()\n",
    "\n",
    "# Print the results to compare\n",
    "print(f\"Total rows in df1: {num_rows_df1}\")\n",
    "print(f\"Total rows in df2: {num_rows_df2}\")\n",
    "print(f\"Total rows in df3: {num_rows_df3}\")\n",
    "print(f\"Total rows in df4: {num_rows_df4}\")\n",
    "print(f\"Total rows in all partitions: {total_rows_partitions}\")\n",
    "print(f\"Total rows in combined DataFrame: {total_rows_combined}\")\n",
    "\n",
    "# Check if the sums match\n",
    "if total_rows_partitions == total_rows_combined:\n",
    "    print(\"The sum of rows from all partitions matches the total rows in the combined DataFrame.\")\n",
    "else:\n",
    "    print(\"Mismatch in row counts. Please check the data loading and concatenation steps.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d43f8c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few rows of the combined DataFrame:\n",
      "  VendorID tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  trip_distance RatecodeID store_and_fwd_flag PULocationID DOLocationID payment_type  fare_amount  extra  mta_tax  tip_amount  tolls_amount  improvement_surcharge  total_amount  congestion_surcharge\n",
      "0      1.0  2021-01-01 00:30:10   2021-01-01 00:36:12             1.00           2.10        1.0                  N        142.0         43.0          2.0         8.00   3.00     0.50        0.00          0.00                   0.30         11.80                  2.50\n",
      "1      1.0  2021-01-01 00:51:20   2021-01-01 00:52:19             1.00           0.20        1.0                  N        238.0        151.0          2.0         3.00   0.50     0.50        0.00          0.00                   0.30          4.30                  0.00\n",
      "2      1.0  2021-01-01 00:43:30   2021-01-01 01:11:06             1.00          14.70        1.0                  N        132.0        165.0          1.0        42.00   0.50     0.50        8.65          0.00                   0.30         51.95                  0.00\n",
      "3      1.0  2021-01-01 00:15:48   2021-01-01 00:31:01             0.00          10.60        1.0                  N        138.0        132.0          1.0        29.00   0.50     0.50        6.05          0.00                   0.30         36.35                  0.00\n",
      "4      2.0  2021-01-01 00:31:49   2021-01-01 00:48:21             1.00           4.94        1.0                  N         68.0         33.0          1.0        16.50   0.50     0.50        4.06          0.00                   0.30         24.36                  2.50\n",
      "Summary of the combined DataFrame:\n",
      "<class 'dask.dataframe.core.DataFrame'>\n",
      "Columns: 18 entries, VendorID to congestion_surcharge\n",
      "dtypes: category(1), category(1), category(1), category(1), category(1), category(1), object(2), float64(10)None\n",
      "Basic statistical details of the combined DataFrame:\n",
      "       passenger_count  trip_distance   fare_amount         extra       mta_tax    tip_amount  tolls_amount  improvement_surcharge  total_amount  congestion_surcharge\n",
      "count    29,425,377.00  30,904,072.00 30,904,072.00 30,904,072.00 30,904,072.00 30,904,072.00 30,904,072.00          30,904,072.00 30,904,072.00         30,259,405.00\n",
      "mean              1.43           6.92         13.52          1.05          0.49          2.34          0.39                   0.30         19.70                  2.25\n",
      "std               1.03         698.38        178.99          1.25          0.08          2.83          1.81                   0.04        179.19                  0.78\n",
      "min               0.00           0.00       -758.00         -5.50         -0.55       -333.32        -88.75                  -0.30       -951.00                 -2.50\n",
      "25%               1.00           1.20          7.50          0.00          0.50          1.00          0.00                   0.30         12.80                  2.50\n",
      "50%               1.00           2.15         11.50          0.50          0.50          2.16          0.00                   0.30         17.16                  2.50\n",
      "75%               2.00           4.34         18.50          2.50          0.50          3.50          0.00                   0.30         24.96                  2.50\n",
      "max             112.00     351,613.36    818,283.44         90.06         38.80      1,140.44        956.55                   0.30    818,286.74                  3.00\n",
      "Columns in the combined DataFrame:\n",
      "Index(['VendorID', 'tpep_pickup_datetime', 'tpep_dropoff_datetime', 'passenger_count', 'trip_distance', 'RatecodeID', 'store_and_fwd_flag', 'PULocationID', 'DOLocationID', 'payment_type', 'fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'improvement_surcharge', 'total_amount', 'congestion_surcharge'], dtype='object')\n",
      "Missing values in each column of the combined DataFrame:\n",
      "VendorID                  834028\n",
      "tpep_pickup_datetime           0\n",
      "tpep_dropoff_datetime          0\n",
      "passenger_count          1478695\n",
      "trip_distance                  0\n",
      "RatecodeID               1478695\n",
      "store_and_fwd_flag       1478695\n",
      "PULocationID                   0\n",
      "DOLocationID                   0\n",
      "payment_type              834028\n",
      "fare_amount                    0\n",
      "extra                          0\n",
      "mta_tax                        0\n",
      "tip_amount                     0\n",
      "tolls_amount                   0\n",
      "improvement_surcharge          0\n",
      "total_amount                   0\n",
      "congestion_surcharge      644667\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Show the first few rows of the combined DataFrame to understand what it contains\n",
    "print(\"First few rows of the combined DataFrame:\")\n",
    "print(df_combined.head())\n",
    "\n",
    "# Get a summary of the DataFrame to understand the data types and the non-null counts\n",
    "print(\"Summary of the combined DataFrame:\")\n",
    "print(df_combined.info())\n",
    "\n",
    "# Compute basic statistical details like percentile, mean, std etc. of the DataFrame's numeric columns\n",
    "print(\"Basic statistical details of the combined DataFrame:\")\n",
    "print(df_combined.describe().compute())\n",
    "\n",
    "# List all columns to make sure all expected columns are present\n",
    "print(\"Columns in the combined DataFrame:\")\n",
    "print(df_combined.columns)\n",
    "\n",
    "# Additional: Check for missing values in each column\n",
    "print(\"Missing values in each column of the combined DataFrame:\")\n",
    "print(df_combined.isnull().sum().compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd3a4d1",
   "metadata": {},
   "source": [
    "### Combined DataFrame Overview\n",
    "\n",
    "#### Data Composition\n",
    "The dataset comprises an extensive collection of 30,904,072 taxi trips, detailing various aspects such as pickup and dropoff times, trip distances, fares, and more.\n",
    "\n",
    "#### Data Structure\n",
    "The DataFrame is organized into 18 columns, providing a broad spectrum of information per taxi trip. This includes categorical data like `VendorID` and `RatecodeID`, alongside numerical data such as `passenger_count`, `trip_distance`, and `fare_amount`.\n",
    "\n",
    "#### Data Types\n",
    "A mix of data types optimizes the storage and processing efficiency, with categories used for identifiers and descriptors, and floating-point numbers for quantitative measurements.\n",
    "\n",
    "#### Statistical Summary\n",
    "- **Central Tendencies**: Average metrics show typical urban taxi rides with a trip distance of approximately 6.92 miles and an average fare amount of 13.52 dollars.\n",
    "- **Variability**: Notable variability in data, especially in `fare_amount` and `trip_distance`, highlighted by standard deviations that are large due to extreme values.\n",
    "- **Extremes**: The presence of extreme values, such as a maximum trip distance of over 351,613 miles and a fare exceeding $818,283, indicate potential outliers or data entry errors.\n",
    "\n",
    "#### Missing Data\n",
    "Several key columns, including `VendorID`, `RatecodeID`, and `congestion_surcharge`, have missing entries. Over 1.4 million records lack passenger count information, with similar deficiencies noted for rate code and store and forward flag, impacting the completeness of analyses that depend on these fields.\n",
    "\n",
    "#### Data Integrity and Actionable Insights\n",
    "While the dataset's comprehensive detail makes it a rich resource for analysis, the presence of outliers and missing values necessitates meticulous data cleaning and preprocessing. Addressing these issues will ensure the robustness of subsequent analyses and modeling efforts, enhancing the reliability of derived insights and predictive models.\n",
    "\n",
    "This initial overview sets the stage for targeted data cleaning, exploratory analysis, and further in-depth study to uncover insights and develop models based on the dynamics of taxi trips documented in the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4245900",
   "metadata": {},
   "source": [
    "# 3. Data Cleaning "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0aba610",
   "metadata": {},
   "source": [
    "## 3.1 Handling Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d50d2b",
   "metadata": {},
   "source": [
    "#### 3.1.1 VendorID   \n",
    "vendorID shows the taxi service provider ID, this is not a crucial part of our analysis, however this can indicate whether different service providers have different charging and pricing systems, thus we will not remove the missing values but use mode imputation to fill in the gaps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c86fd324",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_vendor = df_combined['VendorID'].mode().compute()\n",
    "df_combined['VendorID'] = df_combined['VendorID'].fillna(most_common_vendor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d1c218",
   "metadata": {},
   "source": [
    "#### 3.1.2 payment_type\n",
    "payment type has 834028 missing values, as “payment_type”, information from the knowledge suggests that tips are automatically filled for credit card payments, if “tips” are greater than 0, it is inferred that credit card payment was used. Any remaining missing values are filled with the “unknown” option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "efe977d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0        22480190\n",
       "2.0         6669962\n",
       "0.0          644667\n",
       "Unknown      460442\n",
       "1            373586\n",
       "3.0          154148\n",
       "4.0          121073\n",
       "5.0               4\n",
       "Name: payment_type, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Infer the payment type based on tip amount:\n",
    "df_combined['payment_type'] = df_combined.apply(\n",
    "    lambda row: '1' if pd.isna(row['payment_type']) and row['tip_amount'] > 0 else row['payment_type'],\n",
    "    axis=1,\n",
    "    meta=('payment_type', 'object')  # specifying the meta is important for Dask to know the output format\n",
    ")\n",
    "\n",
    "# Fill remaining missing values in 'payment_type' with 'Unknown'\n",
    "df_combined['payment_type'] = df_combined['payment_type'].fillna('Unknown')\n",
    "\n",
    "# Calculate and display the updated counts of each payment type\n",
    "df_combined['payment_type'].value_counts().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b8ce1f",
   "metadata": {},
   "source": [
    "#### 3.1.3 congestion_surcharge     \n",
    "\n",
    "for congestion surcharge we see that we have  644667 missing values. This surcharge is applied based on specific criteria related to the time and location of the trip, potentially affecting the total fare. \n",
    "\n",
    "We can assume that since this field is empty that this criteria were not met and include 0 values for simlicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2d32a111",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined['congestion_surcharge'] = df_combined['congestion_surcharge'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c6fc26",
   "metadata": {},
   "source": [
    "\n",
    "#### 3.1.4 Analysis of Missing Data Patterns\n",
    "\n",
    "The fact that the missing values count is exactly the same for `passenger_count`, `RatecodeID`, and `store_and_fwd_flag` suggests a few potential scenarios:\n",
    "\n",
    "- **Data Collection Issue**: \n",
    "  The missing values might indicate a systematic error in how the data was collected, processed, or extracted. Perhaps there was an issue with the data collection equipment or software in certain taxis that failed to record these specific details.\n",
    "\n",
    "- **Record Integrity**: \n",
    "  These fields could be missing for entire trips, implying that some records are partially complete. This might happen if, for instance, a technical glitch occurred at the start or end of these trips, affecting multiple data fields simultaneously.\n",
    "\n",
    "- **Data Entry Protocol**: \n",
    "  The taxi meters or systems that log this information might have a unified data entry procedure that skips multiple fields when one critical piece of information is unavailable. For example, if the meter does not start properly and doesn’t log `RatecodeID`, it might also skip logging `passenger_count` and `store_and_fwd_flag`.\n",
    "\n",
    "#### Investigating Further:\n",
    "To address and confirm the root cause, you might consider the following steps:\n",
    "\n",
    "- **Cross-Validation with Other Fields**: \n",
    "  Check if these records with missing values in the three fields also show anomalies or patterns in other data points like `fare_amount`, `trip_distance`, or timestamps. This could help confirm if the trips are entirely corrupt or if only certain aspects are affected.\n",
    "\n",
    "- **Temporal Analysis**: \n",
    "  Analyze the timestamps (`tpep_pickup_datetime` and `tpep_dropoff_datetime`) of these records to see if the missing data occurred during specific periods, which might indicate a temporary system issue.\n",
    "\n",
    "- **Source Data Check**: \n",
    "  If possible, review the raw data or communicate with the data provider to understand potential reasons for these patterns. There might be logs or metadata that explain anomalies in data collection.\n",
    "\n",
    "#### Handling Strategy:\n",
    "Depending on the investigation outcomes, you might choose to:\n",
    "\n",
    "- **Exclude the Affected Records**: \n",
    "  If the records are found to be unreliable or significantly incomplete, consider removing them from the analysis to maintain data integrity.\n",
    "\n",
    "- **Impute Conservatively**: \n",
    "  If the missing data does not compromise the rest of the record's integrity, we will impute these missing fields based on typical assumptions (e.g., most common values) or predictive models if other related data fields are available and reliable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eabec069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamp Statistics:\n",
      "count                    1478695\n",
      "unique                   1139451\n",
      "top       09/15/2021 05:22:00 PM\n",
      "freq                          25\n",
      "Name: tpep_pickup_datetime, dtype: object\n",
      "\n",
      "Fare and Distance Statistics:\n",
      "       fare_amount  trip_distance\n",
      "count 1,478,695.00   1,478,695.00\n",
      "mean         25.50          83.59\n",
      "std          16.13       3,186.98\n",
      "min        -134.22           0.00\n",
      "25%          13.20           2.01\n",
      "50%          21.07           4.15\n",
      "75%          33.96           8.83\n",
      "max       3,554.70     351,613.36\n"
     ]
    }
   ],
   "source": [
    "# Check if the same rows are missing these fields\n",
    "missing_data_rows = df_combined[\n",
    "    df_combined['passenger_count'].isnull() & \n",
    "    df_combined['RatecodeID'].isnull() & \n",
    "    df_combined['store_and_fwd_flag'].isnull()\n",
    "]\n",
    "\n",
    "# Compute to bring the result to pandas DataFrame for easier manipulation and viewing\n",
    "missing_data_rows = missing_data_rows.compute()\n",
    "\n",
    "# Analyze if these rows have any temporal patterns or other commonalities\n",
    "print(\"Timestamp Statistics:\")\n",
    "print(missing_data_rows['tpep_pickup_datetime'].describe())\n",
    "\n",
    "print(\"\\nFare and Distance Statistics:\")\n",
    "print(missing_data_rows[['fare_amount', 'trip_distance']].describe())\n",
    "\n",
    "# Optionally, drop these rows if deemed unreliable\n",
    "df_combined_cleaned = df_combined.dropna(subset=['passenger_count', 'RatecodeID', 'store_and_fwd_flag'])\n",
    "\n",
    "# Compute the cleaned DataFrame if you need a pandas DataFrame, else you can keep it lazy in Dask\n",
    "# df_combined_cleaned = df_combined_cleaned.compute()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e10b991",
   "metadata": {},
   "source": [
    "#### Summary\n",
    "\n",
    "The analysis of records with missing `passenger_count`, `RatecodeID`, and `store_and_fwd_flag` fields reveals several important characteristics and potential data quality issues:\n",
    "\n",
    "#### Timestamp Statistics\n",
    "- **Count of Missing Records**: 1,478,695\n",
    "- **Unique Pickup Times**: 1,139,451\n",
    "- **Most Frequent Pickup Time**: `09/15/2021 05:22:00 PM`, occurring 25 times.\n",
    "\n",
    "This distribution indicates that missing data instances are not isolated to specific events but are spread across various times, suggesting systematic data collection issues.\n",
    "\n",
    "#### Fare and Distance Statistics\n",
    "- **Mean Fare**: 25.50, indicating higher than average fares for these records.\n",
    "- **Standard Deviation of Fare**: 16.13, showing a wide range of fare amounts.\n",
    "- **Fare Range**: From -134.22 (indicating refunds or errors) to $3,554.70 (suggesting outliers or very long trips).\n",
    "- **Mean Trip Distance**: 83.59 miles, much higher than typical urban taxi trips.\n",
    "- **Standard Deviation of Distance**: 3,186.98, confirming significant variability and the presence of extreme outliers.\n",
    "- **Distance Range**: From 0 miles (potential cancellations) to 351,613.36 miles (clear data entry errors).\n",
    "\n",
    "#### Conclusions and Recommendations for Data Cleaning\n",
    "The analysis suggests significant integrity issues with this subset of the data:\n",
    "- **Extreme Values**: The records with missing values are associated with unusually high fares and distances, likely indicating outliers or erroneous entries.\n",
    "- **Cleaning Actions**:\n",
    "  - **Remove Extreme Outliers**: Apply thresholds to exclude implausibly high fares and distances.\n",
    "  - **Correct Negative Values**: Investigate and likely remove records with negative fares or distances as they are not valid transactions.\n",
    "  - **Conservative Imputation**: For records that appear otherwise normal, consider imputing missing values based on typical values or predictive models, ensuring other related data fields are reliable.\n",
    "  - **Refine Temporal Analysis**: Further investigate if data issues are concentrated during specific times or conditions to better understand underlying causes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16406d4",
   "metadata": {},
   "source": [
    "##### 3.1.4 Analysis of Missing Data Patterns - handling missing values \n",
    "\n",
    "As passanger count is a crucial feature we belive could be useful for our analysis we will  “passenger count”, we will replace the 0 values with the central tendency measure, which is the median of that feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033c807c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the median of passenger_count\n",
    "median_passenger_count = df_combined['passenger_count'].quantile(0.5).compute()\n",
    "\n",
    "# Replace zero values with the median\n",
    "df_combined['passenger_count'] = df_combined['passenger_count'].mask(df_combined['passenger_count'] == 0, median_passenger_count)\n",
    "\n",
    "# Confirming the replacement\n",
    "print(\"Number of zero values replaced with median:\", (df_combined['passenger_count'] == median_passenger_count).sum().compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d28ac96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the percentage of cases where trip_type is 1 for each RatecodeID\n",
    "trip_type_percentage = df_combined.groupby('RatecodeID')['trip_type'].apply(lambda x: (x == 1).mean().compute())\n",
    "\n",
    "# Print the results\n",
    "print(trip_type_percentage)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e71f581",
   "metadata": {},
   "source": [
    "This code will group the DataFrame by \"RatecodeID\" and calculate the percentage of cases where \"trip_type\" is 1 for each group. You can then examine the results to verify if the observation holds true. If the percentages are close to 99% for RatecodeID values 1, 2, and 3, then the observation is confirmed. If not, further investigation may be needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7a1698",
   "metadata": {},
   "source": [
    "store_and_fwd_flag - this is not an important feature for our analysis thus we can drop this collumn as we are not interested in the data storage and change. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a44191",
   "metadata": {},
   "source": [
    "### Removing extreme outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66eb04e",
   "metadata": {},
   "source": [
    "For our dataset we have features that we can pre-determine for our research purposes what their values can be.\n",
    "\n",
    "VendorID                  834028 - \n",
    "tpep_pickup_datetime           0 - within 2021 \n",
    "tpep_dropoff_datetime          0 - within 2021\n",
    "passenger_count          1478695 - more than 1 less than 6 as per regulations no more than 4 people can sit in a taxi but to ensure that we include all vehicle types and instances where there might be children the accaptable number can be 6. \n",
    "trip_distance                  0 - as we want to creat model for taxis within the city we want to bind the data to be inside the city and maximum to ariport.\n",
    "RatecodeID               1478695 - no limit other than the fact that has to be from 1 to 6 and 4 is nassau county which is outside of new york and 3 is newark which is also outside the city. \n",
    "store_and_fwd_flag       1478695 - this field indicates if there were any technical problems during data collection, which is irrelavant to our study purposes.\n",
    "Below 2 features have to be bound based on NYC taxi zones provided in another CSV file which we will use to filter out drop off or pickup locations outside NYC for now lets use the code they have to be between 1- 262 \n",
    "PULocationID                   0 - \n",
    "DOLocationID                   0 - \n",
    "payment_type              834028 - from 0 to 6\n",
    "fare_amount                    0 more than 0 \n",
    "extra                          0 \n",
    "mta_tax                        0 \n",
    "tip_amount                     0\n",
    "tolls_amount                   0\n",
    "improvement_surcharge          0\n",
    "total_amount                   0 not minus\n",
    "congestion_surcharge      644667 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
